data<- read.csv("wineQualityN.csv")
View(data)
library(tree) #to fit trees
library(randomForest) #for random forests (and bagging)
library(gbm) #for boosting
library(ggplot2) #for any graphics we may create
library(ROCR) #for plots
library(boot) #for cv.glm function
library(MASS) #for lda function
library(ipred) #for cv/bagging/etc. functions
library(car)
install.packages(c("gbm", "ipred", "ROCR", "tree"))
library(tree) #to fit trees
library(randomForest) #for random forests (and bagging)
library(gbm) #for boosting
library(ggplot2) #for any graphics we may create
library(ROCR) #for plots
library(boot) #for cv.glm function
library(MASS) #for lda function
library(ipred) #for cv/bagging/etc. functions
library(car)
#seeing what pairs look to be generally correlated
pairs(data[,2:13], lower.panel = NULL, main="Scatterplot of Quantitative Variables")
sum(is.na(data))
round(cor(data[,2:13]),3)
glimpse(data)
library(tidyverse)
glimpse(data)
data <- data %>%
mutate(type = ifelse(type == "white", 1, 0))
View(data)
data <- data %>%
mutate(type = case_when(type == "white" ~ 1,
type == "red" ~ 0,
TRUE ~ NA))
data <- data %>%
mutate(type = case_when(type == "white" ~ 1,
type == "red" ~ 0,
~ NA))
data <- data %>%
mutate(type = case_when(type == "white" ~ 1,
type == "red" ~ 0,
TRUE ~ NA))
data <- read.csv("wineQualityN.csv")
data <- data %>%
mutate(type = case_when(type == "white" ~ 1,
type == "red" ~ 0,
TRUE ~ NA))
data <- data %>%
mutate(type = case_when(type == "white" ~ 1,
type == "red" ~ 0,
TRUE ~ 999))
unique(data$type)
round(cor(data[,2:13]),3)
ggplot(data = data) +
geom_boxplot(mapping = aes(x = quality_cat, y = volatile.acidity)) + theme_classic()
#establishing quality variable to measure if wine is above or below median quality
test <- function(x) {
ifelse(
x<median(data$quality),
0,
1
)}
data$quality_cat <- factor(test(data$quality))
View(data)
ggplot(data = data) +
geom_boxplot(mapping = aes(x = quality_cat, y = volatile.acidity)) + theme_classic()
ggplot(data = data) +
geom_boxplot(mapping = aes(x = alcohol, y = volatile.acidity)) + theme_classic()
ggplot(data = data) +
geom_boxplot(mapping = aes(x = quality_cat, y = alcohol)) + theme_classic()
#setting RNG to match newer R versions
#creating a training data set and a testing data set to determine
#how well the model works
RNGkind(sample.kind = "Rejection") #in order to match newer R RNG
#New R uses rejection not rounding
set.seed(1)
#spliting data into testing and training data
#50-50 split as to not overtrain
sample.data<-sample.int(nrow(data), floor(.50*nrow(data)), replace = F)
train<-data[sample.data, ]
test<-data[-sample.data, ]
#setting the response variable to be the alcohol content in the wine
test.y<-test[,"alcohol"]
#in this section we are trying to use chemical makeup and quality to predict alcohol content
#setting the response variable to be the alcohol content in the wine
test.y<-test[,"alcohol"]
#building a simple classification tree
#
tree.reg.train<-tree(alcohol~.-quality_cat, data=train) # using all vars except quality_cat
summary(tree.reg.train)
plot(tree.reg.train)
text(tree.reg.train, cex=0.75, pretty=0)
tree.pred.test<-predict(tree.reg.train, newdata=test)
mean((tree.pred.test - test.y)^2) # test error rate
mean((tree.pred.test - test.y)^2) # test error rate
#cross validation regression
#using K=10 fold cross validation and pruining
#in order to improve the regression
cv.reg<-cv.tree(tree.reg.train, K=10)
cv.reg
trees.num.class<-cv.reg$size[which.min(cv.reg$dev)]
trees.num.class
plot(cv.reg$size, cv.reg$dev, type='b')
prune.reg<-prune.tree(tree.reg.train, best=trees.num.class)
prune.reg
prune.reg<-prune.tree(tree.reg.train, best=trees.num.class)
prune.reg
plot(prune.reg)
text(prune.reg, cex=0.75, pretty=0)
#prediction based on pruned tree for test data
tree.pred.prune<-predict(prune.reg, newdata=test)
#overall accuracy
mean((tree.pred.prune - test.y)^2)
?lm()
#
model <- lm(alcohol~.-quality_cat, data=train)
#
model <- lm(alcohol~.-quality_cat, data=data)
summary(model)
#bagging is special case of random forest when mtry = number of predictors
#used to reduce variance and increase accuracy through resampling
names(train)
bag.reg<-randomForest(alcohol~.-quality_cat, data=train, mtry=12, importance=TRUE)
library(randomForest) #for random forests (and bagging)
install.packages("randomForest")
bag.reg <- randomForest(alcohol~.-quality_cat, data=train, mtry=12, importance=TRUE)
library(randomForest) #for random forests (and bagging)
install.packages("randomForest")
#starting with logistic regression model
LogisticRegression <- glm(quality_cat ~ .-quality, data = train, family = "binomial")
summary(LogisticRegression)
vif(LogisticRegression)
data$density <- NULL
LogTest<-round(predict(LogisticRegression, newdata=test, type="response"))
table(test.y, LogTest)
mean(LogTest != test.y)
par(mfrow=c(2,2))
plot(LogisticRegression)
mod <- glm(quality_cat ~ .-quality, data = train, family = "binomial")
cooksd <- cooks.distance(mod)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
mod <- glm(quality_cat ~ .-quality, data = train, family = "binomial")
cooksd <- cooks.distance(mod)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
#this time predicting for the quality of the wine
#strating off again with a simple regression and classification tree
test.y <- test[,"quality_cat"]
#this time predicting for the quality of the wine
#strating off again with a simple regression and classification tree
test.y <- test[,"quality_cat"]
tree.class.train<-tree(quality_cat~.-quality, data=train)
summary(tree.class.train)
plot(tree.class.train)
text(tree.class.train, cex=0.75, pretty=0,main="Figure 4.1.4")
tree.pred.test<-predict(tree.class.train, newdata=test, type="class")
#confusion matrix for test data
table(test.y, tree.pred.test)
mean(tree.pred.test != test.y)
summary(LogisticRegression)
#volatile acidity, residual sugar, sulfer dioxide (free + total), sulphates and alcohol are significant
vif(LogisticRegression)
?vif
View(data)
data$density <- NULL
LogTest<-round(predict(LogisticRegression, newdata=test, type="response"))
table(test.y, LogTest)
mean(LogTest != test.y)
LogTest
mean(LogTest != test.y)
table(test.y, LogTest)
LogTest != test.y
table(test.y, LogTest)
mean(LogTest != test.y)
mean(LogTest != test.y, na.rm = TRUE)
mean(LogTest != test.y, na.rm = TRUE)
par(mfrow=c(2,2))
plot(LogisticRegression)
1 - mean(LogTest != test.y, na.rm = TRUE)
test.y <- test[,"quality_cat"]
tree.class.train<-tree(quality_cat~.-quality, data=train)
summary(tree.class.train)
#moving on to creating a regression tree based on quality
test.y <- test[,"quality_cat"]
tree.class.train<-tree(quality_cat~.-quality, data=train)
summary(tree.class.train)
plot(tree.class.train)
text(tree.class.train, cex=0.75, pretty=0,main="Figure 4.1.4")
tree.pred.test<-predict(tree.class.train, newdata=test, type="class")
#confusion matrix for test data
table(test.y, tree.pred.test)
mean(tree.pred.test != test.y)
1 - mean(tree.pred.test != test.y)
#using cross validation and pruning to attempt to improve the tree
cv.class<-cv.tree(tree.class.train, K=10, FUN = prune.misclass)
cv.class
trees.num.class<-cv.class$size[which.min(cv.class$dev)]
trees.num.class
plot(cv.class$size, cv.class$dev, type='b',main="Figure 4.2.1",ylab='Deviance',xlab="Size")
prune.class <-prune.tree(tree.class.train, best=3)
summary(prune.class)
plot(prune.class)
text(prune.class, cex=0.75, pretty=0)
#prediction based on pruned tree for test data
tree.pred.prune<-predict(prune.class, newdata=test, type="class")
#overall accuracy
mean(tree.pred.prune != test.y)
#overall accuracy
1 - mean(tree.pred.prune != test.y)
names(train)
bag.class<-randomForest(quality_cat~.-quality, data=train, mtry=12, importance=TRUE)
#importance measures of predictors
importance(bag.class)
